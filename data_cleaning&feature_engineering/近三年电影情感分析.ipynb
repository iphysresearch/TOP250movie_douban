{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3895f39",
   "metadata": {},
   "source": [
    "## 导入数据，数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca1ca9",
   "metadata": {},
   "source": [
    "载入包文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09ce9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "# %load_ext scalene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0ca6b",
   "metadata": {},
   "source": [
    "载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee42ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电影数目： 2060\n"
     ]
    }
   ],
   "source": [
    "# 载入电影数据\n",
    "data_item = pd.read_csv('../douban_movie/data/movies.csv')\n",
    "print('电影数目：' ,data_item.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b193540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>陈奕迅线上慈善演唱会 Live Is So Much Better With Music E...</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>岚ARAFES2020at国立竞技场 ARASHI ARAFES 2020 at Natio...</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>This is 嵐 LIVE</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>宇多田光Laugher in the Dark 2018 巡回演唱会 Hikaru Utad...</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>星野源巡回演唱会：流行病毒 GEN HOSHINO STADIUM TOUR “POP VI...</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>山河令生来知己演唱会</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>安德烈·波切利米兰大教堂空场独唱 Andrea Bocelli : Music for Ho...</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Funtastic Babii Fanmeeting</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>TME Live 吴青峰「16叶」线上演唱会 TME Live 吳青峰「16葉」線上演唱會</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>鱼丁糸Follow Me线上演唱会</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  rate\n",
       "1582  陈奕迅线上慈善演唱会 Live Is So Much Better With Music E...   9.7\n",
       "1765  岚ARAFES2020at国立竞技场 ARASHI ARAFES 2020 at Natio...   9.6\n",
       "1841                                     This is 嵐 LIVE   9.6\n",
       "1071  宇多田光Laugher in the Dark 2018 巡回演唱会 Hikaru Utad...   9.5\n",
       "1223  星野源巡回演唱会：流行病毒 GEN HOSHINO STADIUM TOUR “POP VI...   9.5\n",
       "1919                                         山河令生来知己演唱会   9.5\n",
       "1497  安德烈·波切利米兰大教堂空场独唱 Andrea Bocelli : Music for Ho...   9.4\n",
       "1770                         Funtastic Babii Fanmeeting   9.4\n",
       "1807      TME Live 吴青峰「16叶」线上演唱会 TME Live 吳青峰「16葉」線上演唱會   9.4\n",
       "1662                                  鱼丁糸Follow Me线上演唱会   9.4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总评分最高的前10部电影\n",
    "data_item.sort_values('rate', ascending=False)[['title','rate']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8e4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_expand(data, column, list_values):\n",
    "    for cl in list_values:\n",
    "        tt = data_item[column].apply(lambda x: str(x)).str.contains('\\W'+cl+'\\W')\n",
    "        uu = data_item[column].apply(lambda x: str(x)).str.contains('^'+cl+'$')\n",
    "        ee = data_item[column].apply(lambda x: str(x)).str.contains(cl+'\\s')\n",
    "        ff = data_item[column].apply(lambda x: str(x)).str.contains('\\s'+cl)\n",
    "#         print('cl', cl)\n",
    "#         print('tt', tt)\n",
    "#         print('uu', uu)\n",
    "#         print('ee', ee)\n",
    "        cl_ = tt | uu| ee | ff\n",
    "        cl_ *= 1\n",
    "#         print('c1_', cl_)\n",
    "        data['%s_%s' %(column ,cl)] = cl_\n",
    "#         print('data', data)\n",
    "        \n",
    "class_movie = ['剧情','爱情','喜剧','科幻','动作','悬疑','犯罪','恐怖','青春'\n",
    "               ,'励志','战争','文艺','黑色幽默','传记','情色','暴力','音乐','家庭']\n",
    "country_movie = ['大陆','美国','香港','台湾','日本','韩国','英国','法国','德国','意大利','西班牙','印度','泰国','俄罗斯'\n",
    " ,'伊朗','加拿大','澳大利亚','爱尔兰','瑞典','巴西','丹麦']\n",
    "\n",
    "def get_values_list(data, column, sep=None):\n",
    "    Language_values=[]\n",
    "    def countLANG(Languages):\n",
    "        for language in Languages:\n",
    "            language = language.strip()\n",
    "            if language in Language_values: \n",
    "                continue\n",
    "            else:\n",
    "                Language_values.append(language)\n",
    "    if sep:\n",
    "        pd.DataFrame(data[column].str.split(sep))[column].apply(countLANG);\n",
    "    else:\n",
    "        data[column].apply(countLANG);\n",
    "    return Language_values\n",
    "\n",
    "def Paiming(data, column, list_values):\n",
    "    column_expand(data, column, list_values)\n",
    "    df = pd.DataFrame( \n",
    "        {'数目':[data['%s_%s' %(column, p)].sum() for p in list_values]}\n",
    "        , index=list_values).sort_values('数目', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18dfd3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       数目\n",
      "爱情    139\n",
      "科幻    138\n",
      "悬疑    132\n",
      "动作    129\n",
      "喜剧    121\n",
      "恐怖    108\n",
      "剧情     68\n",
      "音乐     44\n",
      "家庭     44\n",
      "传记     22\n",
      "战争     17\n",
      "犯罪     17\n",
      "情色     11\n",
      "青春      0\n",
      "文艺      0\n",
      "黑色幽默    0\n",
      "暴力      0\n",
      "励志      0\n"
     ]
    }
   ],
   "source": [
    "# 最受欢迎的电影类别排名\n",
    "movie_type_list = Paiming(data_item, 'type', class_movie)\n",
    "print(movie_type_list)\n",
    "# 根据结果显示发现在2019-2021年三年的时间里爱情、科幻、悬疑、动作、喜剧 类型的电影，其中最受欢迎的是爱情题材的电影"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4a2cb",
   "metadata": {},
   "source": [
    "# 词云分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e91995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import jieba   # 分词包\n",
    "import numpy\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rc('figure', figsize = (14, 7))\n",
    "matplotlib.rc('font', size = 14)\n",
    "matplotlib.rc('axes', grid = False)\n",
    "matplotlib.rc('axes', facecolor = 'white')\n",
    "from wordcloud import WordCloud # 词云包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be451d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/j9/x9wd0pm15rgd97b0jsp8sk100000gn/T/jieba.cache\n",
      "Loading model cost 0.352 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['阔别',\n",
       " '三十多年',\n",
       " '同学',\n",
       " '生活',\n",
       " '不同',\n",
       " '地方',\n",
       " '才华横溢',\n",
       " '事业有成',\n",
       " '投资',\n",
       " '有方',\n",
       " '商海',\n",
       " '弄潮',\n",
       " '固守',\n",
       " '田园风光',\n",
       " '难舍',\n",
       " '故乡',\n",
       " '真的']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut_word(content):\n",
    "    # 导入、分词\n",
    "    segment=[]\n",
    "    for line in content: \n",
    "        try:\n",
    "            segs = jieba.lcut(line) # jiaba.lcut() \n",
    "#             print(segs)\n",
    "            for seg in segs:\n",
    "                if len(seg)>1 and seg!='\\r\\n':\n",
    "                    segment.append(seg)\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "#     print('cut_word', segment[:1000])\n",
    "    return segment\n",
    "cut_word(['阔别三十多年的同学，生活在不同的地方，有才华横溢事业有成的，有投资有方商海弄潮的，也有固守田园风光难舍故乡,真的好'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca029c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(segment):\n",
    "    # 去停用词\n",
    "    words_df = pd.DataFrame({'segment':segment})\n",
    "    stopwords = pd.read_csv(\"../stopwords.txt\" \n",
    "                          ,index_col=False\n",
    "                          ,quoting=3\n",
    "                          ,sep=\"\\t\"\n",
    "                          ,names=['stopword']\n",
    "                          ,encoding='utf-8') # quoting=3 全不引用    \n",
    "    words_df=words_df[~words_df.segment.isin(stopwords.stopword)]\n",
    "    return words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171e3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "def get_word_count(words_df):\n",
    "    words_stat = words_df.groupby('segment').agg(计数=pd.NamedAgg(column='segment', aggfunc='size')).reset_index().sort_values(by='计数', ascending=False)\n",
    "    return words_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502eec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "def createHist(x_data):\n",
    "#     ax1.hist(x_data, histtype='stepfilled', bins=200, color = y1_color)\n",
    "    plt.bar(x_width,y_data,lw=0.5,fc=\"r\",width=0.3,label=\"Phone\")\n",
    "    ax1.set_title(title)\n",
    "    plt.ylabel('Number')\n",
    "    plt.xlabel('High frequency words of each movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01af624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词云展示\n",
    "import imageio\n",
    "matplotlib.rcParams['figure.figsize']=(10.0,10.0)\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "def createWordCloud(imgPath, words_stat={}):\n",
    "    bimg=imageio.imread('cover.jpg')\n",
    "    wordcloud=WordCloud(background_color=\"white\",mask=bimg,font_path='../simhei.ttf',max_font_size=200)\n",
    "    word_frequence={x[0]:x[1] for x in words_stat.head(1000).values}\n",
    "    wordcloud=wordcloud.fit_words(word_frequence)\n",
    "    bimgColors=ImageColorGenerator(bimg)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordcloud.recolor(color_func=bimgColors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee81697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordCloudByGenre(content, imgPath='cover.jpg'):\n",
    "    segment = cut_word(content)\n",
    "#     print('segment', segment)\n",
    "#     segment = tf_idf_words(segment)\n",
    "#     print('segment', segment)\n",
    "    words_df = remove_stopword(segment)\n",
    "    words_stat = get_word_count(words_df)\n",
    "    createWordCloud(imgPath,words_stat=words_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ceef0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取影评数据集\n",
    "data_com = pd.read_csv('../douban_movie/data/comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c4cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并电影短评\n",
    "movie_comment_data = pd.merge(data_item, data_com, on=\"movie_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed545ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['爱情', '科幻', '悬疑', '动作', '喜剧'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 对最受欢迎的前五条电影类别进行分析\n",
    "movie_top_five = movie_type_list.head(5).index\n",
    "# movie_type={x: x.index for x in movie_type_list}\n",
    "print(movie_top_five)\n",
    "# # 获取方式某个题材的影评\n",
    "def get_comment(type):\n",
    "    flag = movie_comment_data['type'].apply(lambda x: str(x)).str.contains(type)\n",
    "    content = movie_comment_data[flag].content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b1c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = get_comment('爱情').values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fdadf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "      segment    计数\n",
      "38113      故事  4301\n",
      "22459      喜欢  2864\n",
      "28648      导演  2595\n",
      "26113      女主  1784\n",
      "46471      演员  1745\n",
      "46536      演技  1662\n",
      "49637      男主  1650\n",
      "9542       不错  1490\n",
      "26679      好看  1454\n",
      "59741      角色  1431\n"
     ]
    }
   ],
   "source": [
    "segment = cut_word(comment)\n",
    "words_df = remove_stopword(segment)\n",
    "words_stat = get_word_count(words_df).head(10)\n",
    "print(words_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d9dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词云展示最受欢迎的五个类型评论\n",
    "\n",
    "# createWordCloudByGenre(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ea69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f50a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment = get_comment('科幻').values.tolist()\n",
    "# segment = cut_word(comment)\n",
    "# words_df = remove_stopword(segment)\n",
    "# words_stat = get_word_count(words_df).head(10)\n",
    "# print(words_stat)\n",
    "# createWordCloudByGenre(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f91a998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment = get_comment('悬疑').values.tolist()\n",
    "# segment = cut_word(comment)\n",
    "# words_df = remove_stopword(segment)\n",
    "# words_stat = get_word_count(words_df).head(10)\n",
    "# print(words_stat)\n",
    "# createWordCloudByGenre(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d565b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment = get_comment('动作').values.tolist()\n",
    "# createWordCloudByGenre(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6379d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment = get_comment('喜剧').values.tolist()\n",
    "# segment = cut_word(comment)\n",
    "# words_df = remove_stopword(segment)\n",
    "# words_stat = get_word_count(words_df).head(10)\n",
    "# print(words_stat)\n",
    "# createWordCloudByGenre(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b70ef",
   "metadata": {},
   "source": [
    "## 2.情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4504b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snownlp import SnowNLP\n",
    "# # 读取评论数据\n",
    "# df = pd.read_csv('../douban_movie/data/comment.csv')\n",
    "# df = df.dropna(subset=['content'])\n",
    "# df['segments'] = df['content'].apply(lambda x: (SnowNLP(x).sentiments))\n",
    "# # df['segments'] = df['content'].apply(lambda x: print(x))\n",
    "# # print(df['segments'])\n",
    "# # print(SnowNLP('asdad').sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "299eebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('comment-segments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5048b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../douban_movie/data/comment-segments.csv')\n",
    "df = pd.merge(data_item, df, on=\"movie_id\")\n",
    "# print(len(df['id']))\n",
    "def getRate(df,type):\n",
    "    positive = ['力荐', '推荐']\n",
    "    middle = ['还行']\n",
    "    negative = ['较差', '很差']\n",
    "    praise_flag = df['degree'].apply(lambda x: x in positive)\n",
    "    middle_flag = df['degree'].apply(lambda x: x in middle)\n",
    "    negative_flag = df['degree'].apply(lambda x: x in negative)\n",
    "    number_of_praise = len(df[praise_flag])\n",
    "    number_of_middle = len(df[middle_flag])\n",
    "    number_of_negative = len(df[negative_flag])\n",
    "    total = len(df['id'])\n",
    "    print('-----------{type}---------'.format(type=type))\n",
    "    print('total', total)\n",
    "    print('好评', number_of_praise)\n",
    "    print('差评', number_of_negative)\n",
    "    print('中评', number_of_middle)\n",
    "    print('好评率', number_of_praise/total)\n",
    "    print('差评率', number_of_negative/total)\n",
    "    print('中评率', number_of_middle/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8e66c79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "try:\n",
    "    words = pd.read_csv('./data/data3.csv')\n",
    "except FileNotFoundError:\n",
    "    arr = df.content\n",
    "    data_set = []\n",
    "    for content in arr:\n",
    "        segment = []\n",
    "    #     jieba.analyse.set_stop_words('../customer-stopword.txt')\n",
    "        segs = jieba.analyse.extract_tags(content, topK=20, allowPOS=('n', 'v'), withWeight=False)\n",
    "#         segs = pseg.cut(content)\n",
    "        stopwords = pd.read_csv(\"../stopwords.txt\" \n",
    "                                  ,index_col=False\n",
    "                                  ,quoting=3\n",
    "                                  ,sep=\"\\t\"\n",
    "                                  ,names=['stopword']\n",
    "                                  ,encoding='utf-8') # quoting=3 全不引用 \n",
    "        for seg in segs:\n",
    "            if  len(seg)>1 and seg!='\\r\\n' and seg not in stopwords.stopword.values.tolist():\n",
    "                segment.append(seg)\n",
    "        data_set.append(segment)\n",
    "    data = pd.DataFrame(data_set)\n",
    "    data.to_csv('./data/data3.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0344a54",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/word2vec3.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m创意\u001b[39m\u001b[38;5;124m'\u001b[39m, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/word2vec.py:779\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary \u001b[38;5;241m=\u001b[39m Word2VecVocab(\n\u001b[1;32m    775\u001b[0m     max_vocab_size\u001b[38;5;241m=\u001b[39mmax_vocab_size, min_count\u001b[38;5;241m=\u001b[39mmin_count, sample\u001b[38;5;241m=\u001b[39msample, sorted_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(sorted_vocab),\n\u001b[1;32m    776\u001b[0m     null_word\u001b[38;5;241m=\u001b[39mnull_word, max_final_vocab\u001b[38;5;241m=\u001b[39mmax_final_vocab, ns_exponent\u001b[38;5;241m=\u001b[39mns_exponent)\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainables \u001b[38;5;241m=\u001b[39m Word2VecTrainables(seed\u001b[38;5;241m=\u001b[39mseed, vector_size\u001b[38;5;241m=\u001b[39msize, hashfxn\u001b[38;5;241m=\u001b[39mhashfxn)\n\u001b[0;32m--> 779\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWord2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbow_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbow_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFAST_VERSION\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/base_any2vec.py:759\u001b[0m, in \u001b[0;36mBaseWordEmbeddingsModel.__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentences, GeneratorType):\n\u001b[1;32m    757\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pass a generator as the sentences argument. Try a sequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    761\u001b[0m         sentences\u001b[38;5;241m=\u001b[39msentences, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[1;32m    762\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    763\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39mcompute_loss)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/base_any2vec.py:943\u001b[0m, in \u001b[0;36mBaseWordEmbeddingsModel.build_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    939\u001b[0m report_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary\u001b[38;5;241m.\u001b[39mprepare_vocab(\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv, update\u001b[38;5;241m=\u001b[39mupdate, keep_raw_vocab\u001b[38;5;241m=\u001b[39mkeep_raw_vocab,\n\u001b[1;32m    941\u001b[0m     trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    942\u001b[0m report_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_memory(vocab_size\u001b[38;5;241m=\u001b[39mreport_values[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_retained_words\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 943\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/word2vec.py:1876\u001b[0m, in \u001b[0;36mWord2VecTrainables.prepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;66;03m# set initial input/projection and hidden weights\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update:\n\u001b[0;32m-> 1876\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weights(hs, negative, wv)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/word2vec.py:1893\u001b[0m, in \u001b[0;36mWord2VecTrainables.reset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;66;03m# randomize weights vector by vector, rather than materializing a huge random matrix in RAM at once\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(wv\u001b[38;5;241m.\u001b[39mvocab)):\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;66;03m# construct deterministic seed from word AND seed argument\u001b[39;00m\n\u001b[0;32m-> 1893\u001b[0m     wv\u001b[38;5;241m.\u001b[39mvectors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseeded_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex2word\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hs:\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msyn1 \u001b[38;5;241m=\u001b[39m zeros((\u001b[38;5;28mlen\u001b[39m(wv\u001b[38;5;241m.\u001b[39mvocab), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1_size), dtype\u001b[38;5;241m=\u001b[39mREAL)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/word2vec.py:1883\u001b[0m, in \u001b[0;36mWord2VecTrainables.seeded_vector\u001b[0;34m(self, seed_string, vector_size)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;124;03m\"\"\"Get a random vector (but deterministic by seed_string).\"\"\"\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;66;03m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[39;00m\n\u001b[0;32m-> 1883\u001b[0m once \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomState\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhashfxn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0xffffffff\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (once\u001b[38;5;241m.\u001b[39mrand(vector_size) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m/\u001b[39m vector_size\n",
      "File \u001b[0;32mmtrand.pyx:184\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_mt19937.pyx:130\u001b[0m, in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=data_set, size=50, window=5, min_count=1, workers=8)\n",
    "model.save(\"./data/word2vec3.model\")\n",
    "model.wv.most_similar('创意', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fb82773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('用力', 0.9325686097145081),\n",
       " ('级别', 0.9115071892738342),\n",
       " ('到位', 0.9104755520820618),\n",
       " ('细节', 0.9037952423095703),\n",
       " ('手法', 0.8996878266334534),\n",
       " ('印象', 0.8994144797325134),\n",
       " ('可惜', 0.8985620141029358),\n",
       " ('美术', 0.8977164030075073),\n",
       " ('功夫', 0.8965245485305786),\n",
       " ('出色', 0.8956985473632812),\n",
       " ('灾难', 0.893528938293457),\n",
       " ('优点', 0.8924545645713806),\n",
       " ('剧本', 0.8902140259742737),\n",
       " ('观众', 0.8898880481719971),\n",
       " ('失望', 0.8860753178596497),\n",
       " ('节奏', 0.8856069445610046),\n",
       " ('版本', 0.8839591145515442),\n",
       " ('大片', 0.8838419914245605),\n",
       " ('难受', 0.8825384974479675),\n",
       " ('舞台', 0.8811585903167725),\n",
       " ('表达', 0.8803365230560303),\n",
       " ('对话', 0.8800433874130249),\n",
       " ('经典', 0.8791768550872803),\n",
       " ('戏剧', 0.8782491683959961),\n",
       " ('欣赏', 0.8781362175941467),\n",
       " ('味道', 0.8778901100158691),\n",
       " ('放到', 0.8777268528938293),\n",
       " ('广告', 0.8770118951797485),\n",
       " ('技巧', 0.8768482208251953),\n",
       " ('无力', 0.8766248822212219)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load('./data/word2vec3.model')\n",
    "model.wv.most_similar('演员', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6bc2d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('亮点', 0.9846416115760803),\n",
       " ('到位', 0.9832262992858887),\n",
       " ('缺点', 0.9822530150413513),\n",
       " ('优点', 0.9816826581954956),\n",
       " ('主线', 0.9811536073684692),\n",
       " ('灾难', 0.980776846408844),\n",
       " ('线索', 0.9770481586456299),\n",
       " ('场面', 0.9768010973930359),\n",
       " ('模仿', 0.9761210083961487),\n",
       " ('动机', 0.9752260446548462),\n",
       " ('痕迹', 0.9742719531059265),\n",
       " ('细节', 0.9735087752342224),\n",
       " ('交代', 0.9723002314567566),\n",
       " ('出色', 0.9710083603858948),\n",
       " ('基调', 0.9700902104377747),\n",
       " ('美术', 0.9694360494613647),\n",
       " ('塑造', 0.9686477780342102),\n",
       " ('影子', 0.9686455130577087),\n",
       " ('对话', 0.968528151512146),\n",
       " ('氛围', 0.9675323367118835),\n",
       " ('沦为', 0.9659230709075928),\n",
       " ('级别', 0.965461015701294),\n",
       " ('味道', 0.9653477668762207),\n",
       " ('切换', 0.9647563099861145),\n",
       " ('还原', 0.9645223021507263),\n",
       " ('无比', 0.9636674523353577),\n",
       " ('文艺', 0.9635986089706421),\n",
       " ('功夫', 0.9620676040649414),\n",
       " ('改编', 0.96144038438797),\n",
       " ('没能', 0.9607720375061035)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('情节', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0810a978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('表演', 0.8574581742286682),\n",
       " ('出色', 0.8331863880157471),\n",
       " ('戏剧', 0.82332444190979),\n",
       " ('影片', 0.8232444524765015),\n",
       " ('团队', 0.8200812935829163),\n",
       " ('出土', 0.8178362250328064),\n",
       " ('到位', 0.8148158192634583),\n",
       " ('大片', 0.8129892945289612),\n",
       " ('体验', 0.8121553063392639),\n",
       " ('经典', 0.8102008700370789),\n",
       " ('影子', 0.80657958984375),\n",
       " ('手法', 0.8056279420852661),\n",
       " ('剧本', 0.8047341704368591),\n",
       " ('美术', 0.8041030764579773),\n",
       " ('技巧', 0.8034223318099976),\n",
       " ('灾难', 0.8012386560440063),\n",
       " ('用力', 0.8007007837295532),\n",
       " ('层面', 0.7997066974639893),\n",
       " ('观众', 0.7997002005577087),\n",
       " ('失望', 0.7994277477264404),\n",
       " ('级别', 0.7982085347175598),\n",
       " ('拍摄', 0.7977235913276672),\n",
       " ('场面', 0.7968255877494812),\n",
       " ('细节', 0.7968000769615173),\n",
       " ('情感', 0.7967529892921448),\n",
       " ('优点', 0.7962159514427185),\n",
       " ('个性', 0.7948406338691711),\n",
       " ('逻辑', 0.7942978143692017),\n",
       " ('舞台', 0.7942209839820862),\n",
       " ('缺点', 0.7930411100387573)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('导演', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41c54592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('调度', 0.9789342284202576),\n",
       " ('刻意', 0.9691569805145264),\n",
       " ('略显', 0.9676828384399414),\n",
       " ('野心', 0.9646788835525513),\n",
       " ('叙事', 0.9610164165496826),\n",
       " ('构图', 0.9607267379760742),\n",
       " ('剧作', 0.9596087336540222),\n",
       " ('饱满', 0.9587934017181396),\n",
       " ('摄影', 0.9587878584861755),\n",
       " ('美学', 0.9547196626663208),\n",
       " ('渲染', 0.9541066288948059),\n",
       " ('新人', 0.9540977478027344),\n",
       " ('碎片', 0.9526451230049133),\n",
       " ('衔接', 0.9522627592086792),\n",
       " ('震撼', 0.9511477947235107),\n",
       " ('沉浸', 0.9466251134872437),\n",
       " ('空洞', 0.9463822245597839),\n",
       " ('廉价', 0.9451215267181396),\n",
       " ('克制', 0.9450045824050903),\n",
       " ('打动', 0.9443309903144836),\n",
       " ('堪称', 0.9438174366950989),\n",
       " ('话剧', 0.943645179271698),\n",
       " ('手持', 0.9436127543449402),\n",
       " ('写实', 0.941469132900238),\n",
       " ('转折', 0.9406062960624695),\n",
       " ('电影节', 0.940559983253479),\n",
       " ('人物形象', 0.9381392002105713),\n",
       " ('编排', 0.9366158843040466),\n",
       " ('融入', 0.9361589550971985),\n",
       " ('松散', 0.9356771111488342)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('镜头', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fdc15b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('痕迹', 0.981736421585083),\n",
       " ('文艺', 0.9762628674507141),\n",
       " ('视觉', 0.9761107563972473),\n",
       " ('还原', 0.9731016159057617),\n",
       " ('摄影', 0.9709812998771667),\n",
       " ('出色', 0.968325138092041),\n",
       " ('美术', 0.967509925365448),\n",
       " ('用心', 0.9657629728317261),\n",
       " ('模仿', 0.9647272825241089),\n",
       " ('切换', 0.9638059139251709),\n",
       " ('氛围', 0.9626973867416382),\n",
       " ('演绎', 0.9614977240562439),\n",
       " ('场景', 0.961313784122467),\n",
       " ('缺点', 0.9610882997512817),\n",
       " ('夸张', 0.9609147906303406),\n",
       " ('刻画', 0.960405170917511),\n",
       " ('剧作', 0.95957350730896),\n",
       " ('尺度', 0.9575209021568298),\n",
       " ('情节', 0.9574722051620483),\n",
       " ('画面', 0.9569975733757019),\n",
       " ('剧本', 0.9565473794937134),\n",
       " ('灾难', 0.9561775326728821),\n",
       " ('改编', 0.9543325304985046),\n",
       " ('沦为', 0.9538736939430237),\n",
       " ('叙述', 0.9538387656211853),\n",
       " ('塑造', 0.9528836607933044),\n",
       " ('气质', 0.951707661151886),\n",
       " ('毛病', 0.9512271881103516),\n",
       " ('魅力', 0.9500613808631897),\n",
       " ('主线', 0.9498776793479919)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('拍摄', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ae02775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('痕迹', 0.981736421585083),\n",
       " ('文艺', 0.9762628674507141),\n",
       " ('视觉', 0.9761107563972473),\n",
       " ('还原', 0.9731016159057617),\n",
       " ('摄影', 0.9709812998771667),\n",
       " ('出色', 0.968325138092041),\n",
       " ('美术', 0.967509925365448),\n",
       " ('用心', 0.9657629728317261),\n",
       " ('模仿', 0.9647272825241089),\n",
       " ('切换', 0.9638059139251709),\n",
       " ('氛围', 0.9626973867416382),\n",
       " ('演绎', 0.9614977240562439),\n",
       " ('场景', 0.961313784122467),\n",
       " ('缺点', 0.9610882997512817),\n",
       " ('夸张', 0.9609147906303406),\n",
       " ('刻画', 0.960405170917511),\n",
       " ('剧作', 0.95957350730896),\n",
       " ('尺度', 0.9575209021568298),\n",
       " ('情节', 0.9574722051620483),\n",
       " ('画面', 0.9569975733757019),\n",
       " ('剧本', 0.9565473794937134),\n",
       " ('灾难', 0.9561775326728821),\n",
       " ('改编', 0.9543325304985046),\n",
       " ('沦为', 0.9538736939430237),\n",
       " ('叙述', 0.9538387656211853),\n",
       " ('塑造', 0.9528836607933044),\n",
       " ('气质', 0.951707661151886),\n",
       " ('毛病', 0.9512271881103516),\n",
       " ('魅力', 0.9500613808631897),\n",
       " ('主线', 0.9498776793479919),\n",
       " ('到位', 0.949418842792511),\n",
       " ('线索', 0.9479015469551086),\n",
       " ('内涵', 0.9476402997970581),\n",
       " ('体验', 0.9475505948066711),\n",
       " ('技巧', 0.947238564491272),\n",
       " ('基调', 0.9463955760002136),\n",
       " ('没能', 0.9461687803268433),\n",
       " ('情感', 0.9445253610610962),\n",
       " ('细节', 0.9438261985778809),\n",
       " ('优点', 0.9436379671096802),\n",
       " ('动机', 0.9435743093490601),\n",
       " ('贯穿', 0.9435269236564636),\n",
       " ('亮点', 0.942991316318512),\n",
       " ('一流', 0.9425015449523926),\n",
       " ('传达', 0.9414514899253845),\n",
       " ('水准', 0.9413636922836304),\n",
       " ('展现', 0.9410372972488403),\n",
       " ('无比', 0.940858781337738),\n",
       " ('气息', 0.9406040906906128),\n",
       " ('掩盖', 0.9405876398086548),\n",
       " ('现实主义', 0.9404982924461365),\n",
       " ('大师', 0.9398022890090942),\n",
       " ('扁平', 0.9378613233566284),\n",
       " ('影子', 0.9375906586647034),\n",
       " ('手法', 0.9374476075172424),\n",
       " ('级别', 0.9372482299804688),\n",
       " ('做成', 0.9369730353355408),\n",
       " ('对话', 0.9359849095344543),\n",
       " ('搭配', 0.9350502490997314),\n",
       " ('传奇', 0.9349775314331055),\n",
       " ('弹劾', 0.9333916902542114),\n",
       " ('观众', 0.9328670501708984),\n",
       " ('借鉴', 0.9326997995376587),\n",
       " ('廉价', 0.932594895362854),\n",
       " ('堪称', 0.9323375225067139),\n",
       " ('提名', 0.9314987659454346),\n",
       " ('体会', 0.931290328502655),\n",
       " ('看不到', 0.9311550855636597),\n",
       " ('解读', 0.9303938150405884),\n",
       " ('功夫', 0.9301790595054626),\n",
       " ('看不出', 0.9297512769699097),\n",
       " ('佩服', 0.9291759729385376),\n",
       " ('主观', 0.9284392595291138),\n",
       " ('场面', 0.9281603097915649),\n",
       " ('讲述', 0.9275730848312378),\n",
       " ('意味', 0.9275147318840027),\n",
       " ('呈现出', 0.9271795153617859),\n",
       " ('擅长', 0.926508903503418),\n",
       " ('专注', 0.926368772983551),\n",
       " ('精彩', 0.9261922836303711),\n",
       " ('压抑', 0.9259175658226013),\n",
       " ('悬念', 0.9257842302322388),\n",
       " ('包袱', 0.9251394867897034),\n",
       " ('讽刺', 0.9250074028968811),\n",
       " ('压缩', 0.9237118363380432),\n",
       " ('震撼', 0.9233627319335938),\n",
       " ('构建', 0.9223095774650574),\n",
       " ('味道', 0.9222991466522217),\n",
       " ('不明', 0.9219979047775269),\n",
       " ('命题', 0.9219264984130859),\n",
       " ('意图', 0.9208883047103882),\n",
       " ('欣赏', 0.9206910133361816),\n",
       " ('视频', 0.9199737906455994),\n",
       " ('缺陷', 0.9185163974761963),\n",
       " ('技法', 0.9172786474227905),\n",
       " ('增添', 0.9171240329742432),\n",
       " ('节制', 0.9161643385887146),\n",
       " ('认同', 0.9159613251686096),\n",
       " ('弥补', 0.9157920479774475),\n",
       " ('主演', 0.9157405495643616),\n",
       " ('碰撞', 0.9146655797958374),\n",
       " ('电视剧', 0.9146138429641724),\n",
       " ('难受', 0.9144323468208313),\n",
       " ('版本', 0.9142478704452515),\n",
       " ('支线', 0.913985013961792),\n",
       " ('眼球', 0.9138415455818176),\n",
       " ('人文', 0.9136871099472046),\n",
       " ('形容', 0.9132866263389587),\n",
       " ('起码', 0.9132252335548401),\n",
       " ('用力', 0.9130071401596069),\n",
       " ('国产', 0.9126716256141663),\n",
       " ('交代', 0.9114057421684265),\n",
       " ('交叉', 0.9112151265144348),\n",
       " ('跳跃', 0.910327672958374),\n",
       " ('姿态', 0.9101070165634155),\n",
       " ('感慨', 0.9098939895629883),\n",
       " ('美学', 0.9096400737762451),\n",
       " ('衔接', 0.90940260887146),\n",
       " ('包装', 0.9091238975524902),\n",
       " ('可笑', 0.9087353348731995),\n",
       " ('落入', 0.9086456298828125),\n",
       " ('口号', 0.9081084728240967),\n",
       " ('法庭', 0.9080847501754761),\n",
       " ('赋予', 0.9078871607780457),\n",
       " ('不乏', 0.9074178338050842),\n",
       " ('生涯', 0.9065943956375122),\n",
       " ('调度', 0.9064542055130005),\n",
       " ('复制', 0.9062812924385071),\n",
       " ('来回', 0.9058461785316467),\n",
       " ('好歹', 0.9048308730125427),\n",
       " ('议题', 0.9040241837501526),\n",
       " ('手持', 0.9038123488426208),\n",
       " ('新人', 0.9035748243331909),\n",
       " ('写出', 0.9034801125526428),\n",
       " ('个性', 0.9032657742500305),\n",
       " ('追逐', 0.9026832580566406),\n",
       " ('标题', 0.9022356271743774),\n",
       " ('舞台', 0.9017961025238037),\n",
       " ('艺术家', 0.9016398787498474),\n",
       " ('表述', 0.9014997482299805),\n",
       " ('不足以', 0.9011016488075256),\n",
       " ('精心', 0.9009876251220703),\n",
       " ('同类', 0.9009860157966614),\n",
       " ('主人公', 0.9006316065788269),\n",
       " ('交错', 0.9005597233772278),\n",
       " ('本土', 0.9001915454864502),\n",
       " ('当做', 0.8997371196746826),\n",
       " ('记忆', 0.8993063569068909),\n",
       " ('暴力', 0.8992559313774109),\n",
       " ('奈何', 0.8992183208465576)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('拍摄', topn=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98504dcc",
   "metadata": {},
   "source": [
    "## 3.主题分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26927004",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '剧情' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m剧情\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/keyedvectors.py:553\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m     mean\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m word)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     mean\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m    555\u001b[0m         all_words\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[word]\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/gensim/models/keyedvectors.py:468\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.word_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m word)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '剧情' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar('剧情', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88759950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    " \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "content_by_type = {}\n",
    "for type in movie_top_five:\n",
    "    flag = df['type'].apply(lambda x: str(x)).str.contains(type)\n",
    "    content = df[flag].content.values.tolist()\n",
    "    content_by_type[type] = content\n",
    "# print(content_by_type['爱情'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aab4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "# 构建词典\n",
    "data_set = []\n",
    "content_X = df.content\n",
    "for content in content_X:\n",
    "    segment = []\n",
    "#     jieba.analyse.set_stop_words('../customer-stopword.txt')\n",
    "    segs = jieba.analyse.extract_tags(content, topK=20, allowPOS=('n'), withWeight=False)\n",
    "#     segs = jieba.lcut(content)\n",
    "    stopwords = pd.read_csv(\"../stopwords.txt\" \n",
    "                              ,index_col=False\n",
    "                              ,quoting=3\n",
    "                              ,sep=\"\\t\"\n",
    "                              ,names=['stopword']\n",
    "                              ,encoding='utf-8') # quoting=3 全不引用 \n",
    "    for seg in segs:\n",
    "        if len(seg)>1 and seg!='\\r\\n' and seg not in stopwords.stopword.values.tolist():\n",
    "            segment.append(seg)\n",
    "    data_set.append(segment)\n",
    "print(data_set[:100])\n",
    "dictionary = corpora.Dictionary(data_set)  # 构建词典\n",
    "# print(dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_set]\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建LDA模型\n",
    "import pprint\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "num_topics = 8\n",
    "# 传统lda词袋30\n",
    "# ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30,random_state = 1)   #分为5个主题,text:文本，已经表示成词袋了。num_topics: 提取的主题数id2word:词典passes:类似于在机器学习中常见的epoch，也就是训练了多少轮;random_state -这是一个种子（如果您想精确地重复训练过程）。\n",
    "# pprint.pprint(ldamodel.print_topics(num_topics=num_topics, num_words=15))  #每个主题输出5个单词\n",
    "\n",
    "#LdaMallet模型 50\n",
    "mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=8, id2word=dictionary)\n",
    "pprint.pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# tfidf 47\n",
    "# tfidf = gensim.models.TfidfModel(corpus)\n",
    "# corpus_tfidf = tfidf[corpus]\n",
    "# ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30,random_state = 1)   #分为5个主题,text:文本，已经表示成词袋了。num_topics: 提取的主题数id2word:词典passes:类似于在机器学习中常见的epoch，也就是训练了多少轮;random_state -这是一个种子（如果您想精确地重复训练过程）。\n",
    "# pprint.pprint(ldamodel.print_topics(num_topics=num_topics, num_words=15))  #每个主题输出5个单词\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_set, dictionary=dictionary, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e12f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ae214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=data_set, size=50, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba29f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('摄影', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('演员', topn=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('特效', topn=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83126a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算困惑度\n",
    "def perplexity(num_topics):\n",
    "    ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=15))\n",
    "    print(ldamodel.log_perplexity(corpus))\n",
    "    return ldamodel.log_perplexity(corpus)\n",
    "#计算coherence\n",
    "def coherence(num_topics):\n",
    "    ldamodel = LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=30,random_state = 1)\n",
    "    print(ldamodel.print_topics(num_topics=num_topics, num_words=10))\n",
    "    ldacm = CoherenceModel(model=ldamodel, texts=data_set, dictionary=dictionary, coherence='c_v')\n",
    "#     print(ldacm.get_coherence())\n",
    "    return ldacm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制主题-coherence曲线，选择最佳主题数\n",
    "x = range(1,15)\n",
    "y = [perplexity(i) for i in x]  #如果想用困惑度就选这个\n",
    "# y = [coherence(i) for i in x]\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('主题数目')\n",
    "plt.ylabel('coherence大小')\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "plt.title('主题-coherence变化情况')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  通过上述主题评估，我们发现可以选择3作为主题个数，接下来我们可以再跑一次模型，设定主题数为5，并输出每个文档最有可能对应的主题\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora, models\n",
    "\n",
    " \n",
    "dictionary = corpora.Dictionary(data_set)  # 构建词典\n",
    "corpus = [dictionary.doc2bow(text) for text in data_set]\n",
    " \n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=6, passes = 30,random_state=1)\n",
    "topic_list=lda.print_topics()\n",
    "pprint.pprint(topic_list)\n",
    "for i in lda.get_document_topics(corpus)[:]:\n",
    "    print('i==',i)\n",
    "    listj=[]\n",
    "    for j in i:\n",
    "        print('j==', j)\n",
    "        listj.append(j[1])\n",
    "#     print(listj)\n",
    "    bz=listj.index(max(listj))\n",
    "#     print(i[bz][0])\n",
    "\n",
    "# for topic in lda.print_topics(num_words = 20):\n",
    "#     termNumber = topic[0]\n",
    "#     print(topic[0], ':', sep='')\n",
    "#     listOfTerms = topic[1].split('+')\n",
    "#     for term in listOfTerms:\n",
    "#         listItems = term.split('*')\n",
    "#         print('  ', listItems[1], '(', listItems[0], ')', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86556dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "# 用pyLDAvis将LDA模式可视化\n",
    "plot =pyLDAvis.gensim_models.prepare(lda,corpus,dictionary)\n",
    "# pyLDAvis.show(plot)\n",
    "# 保存到本地html\n",
    "pyLDAvis.save_html(plot, 'pyLDAvis.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddf61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22a546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
